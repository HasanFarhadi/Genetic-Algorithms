# -*- coding: utf-8 -*-
"""EvolutionStrategy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pkPV3PmpYN0zzcDUl8bv495WVXCqQWOF
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

modelconfig = pd.read_csv("/content/drive/MyDrive/Homework/EC_HW/model_parametres.csv", header=None)

#reading operator parameters from the 'modele_parameter' file
x1step = modelconfig.iloc[0, 1]
x2step = modelconfig.iloc[1, 1]
x1_LearnRate = modelconfig.iloc[2, 1]
x2_LearnRate = modelconfig.iloc[3, 1]
Global_LearnRate = modelconfig.iloc[4, 1]
adaptive_mutation = modelconfig.iloc[5, 1]

recombination_type = modelconfig.iloc[6, 1]

#fitness function
def Schwefel(x):
    x1, x2 = x[:2]
    return (418.9829 * 2 - (x1 * (np.sin(np.sqrt(np.abs(x1)))) + (x2 * np.sin(np.sqrt(np.abs(x2))))))

#plotting the fitness function
r_min, r_max = -500, 500 
xaxis = np.arange(r_min, r_max, 0.1)
yaxis = np.arange(r_min, r_max, 0.1)
x, y = np.meshgrid(xaxis, yaxis)
xy = (x, y)
results = Schwefel(xy)
figure = plt.figure()
axis = figure.gca(projection='3d')
axis.plot_surface(x, y, results, cmap = 'inferno')
plt.show()

#determining wheter a given point is inside our search space or not
def Inside_Bounds(x):
    x1, x2 = x[:2]
    if (x1 < -500) or (x1 > 500):
        return False
    
    if (x2 < -500) or (x2 > 500):
        return False
    
    else:
        return True

#500 random points to start our algorithm from
def InitializeParents(x1step, x2step):
    population = []
    for i in range(500):
        randnum1 = -500 + np.random.rand(1) * 1000
        randnum2 = -500 + np.random.rand(1) * 1000
        randnum = np.array([randnum1, randnum2, x1step, x2step])
        population.append(randnum)
    return population

#selecting the best 100 points from a given population
def Parent_Selection(parent_population):
    chosen = []
    for i in range(100):
        chosen_index = np.random.choice(len(parent_population))
        chosen.append(parent_population[chosen_index])
    return np.array(chosen)

#our mutation and crossover function 
def Mutation(data, Global_LearnRate, x1_LearnRate, x2_Learnrate):
    mutant = []
    x1step = data[2]
    x2step = data[3]
    x1step = x1step * np.exp((Global_LearnRate + x1_LearnRate) * np.random.randn())
    x2step = x2step * np.exp((Global_LearnRate + x2_LearnRate) * np.random.randn())
    mutant.append(data[0] + x1step * np.random.randn())
    mutant.append(data[1] + x2step * np.random.randn())
    mutant.append(data[2])
    mutant.append(data[3])
    return np.asarray(mutant)

def Recombination(data1, data2, recombination_type):
    child = []

    if (recombination_type == 1):
        child.append((data1[0] + data2[0])/2)
        child.append((data1[1] + data2[1])/2)
        child.append((data1[2] + data2[2])/2)
        child.append((data1[3] + data2[3])/2)

    else:
        headortail = np.round(np.random.uniform())
        if (headortail == 1):
            child.append(data1[0])
            child.append(data1[1])
            child.append(data1[2])
            child.append(data1[3])
        else:
            child.append(data2[0])
            child.append(data2[1])
            child.append(data2[2])
            child.append(data2[3])


    return np.asarray(child)

#generating 700 children by applying our implemented mutation and crossover repeatedly on the given parent population 
def Make_Children(chosen_parents, Global_LearnRate, x1_LearnRate, x2_LearnRate, recombination_type):
    children_population = []
    for i in range(len(chosen_parents) * 7):
        parent1index = np.random.choice(len(chosen_parents))
        parent2index = np.random.choice(len(chosen_parents))
        
        parent1 = chosen_parents[parent1index]
        parent2 = chosen_parents[parent2index]

        child = Recombination(parent1, parent2, recombination_type)
        child = Mutation(child, Global_LearnRate, x1_LearnRate, recombination_type)
        if Inside_Bounds(child):
            children_population.append(child)
    
    return np.asarray(children_population)

#selecting 100 of the best children to be the parents of the next generation
def SurvivorSelection(children_population):
    fitness_vals = np.array([Schwefel(x) for x in children_population]).reshape(-1, 1)
    best_children_index = np.argsort(fitness_vals, axis = 0)[:100]
    np.shape(best_children_index)
    best_children = [children_population[i] for i in best_children_index]
    return np.asarray(best_children).reshape(-1, 4)

#the best fitness and its location in a given population
def best_fitness(population):
    fitness_vals = np.array([Schwefel(x) for x in population]).reshape(-1, 1)
    best_fitness = np.sort(fitness_vals, axis=0)[:1]

    argmin = population[np.argmin(fitness_vals)]

    return best_fitness, argmin

lowestfitness = 400
newbest = 1

#using our implemented functions
random_population = np.array(InitializeParents(x1step, x2step)).reshape(-1, 4)
chosen_parents = Parent_Selection(random_population)
iterationcount = 0
for i in range(1000):

    iterationcount += 1

    children_population = Make_Children(chosen_parents, Global_LearnRate, x1_LearnRate, x2_LearnRate, recombination_type)
    chosen_parents = SurvivorSelection(children_population)

    iterationlowest, globalminimum = best_fitness(chosen_parents)
#tuning the mutation step with regard to the success of the last iteration
    if (iterationlowest >= lowestfitness):
        if(adaptive_mutation == 1):
            x1step = x1step / 1.5
            x2step = x2step / 1.5
    else:
        lowestfitness = iterationlowest
        if(adaptive_mutation == 1):
            x1step *= 5
            x1step *= 5
    
    print(f"the best fitness in iteration {iterationcount} : {lowestfitness} at {globalminimum[:2]}")